{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is indeed a fundamental optimization algorithm, especially in training neural networks. Here's a clearer breakdown of the key concepts:\n",
    "\n",
    "• Objective Function: This is the function $J(\\theta)$ that you want to minimize. It represents the error or loss of your model.\n",
    "\n",
    "• Parameters: The parameters $\\theta$ (often denoted as weights in neural networks) are what you adjust during training to minimize the objective function.\n",
    "\n",
    "• Gradient: The gradient $\\nabla J(\\theta)$ is a vector of partial derivatives that points in the direction of the steepest ascent of the function. To minimize $J(\\theta)$, you update the parameters in the opposite direction of the gradient.\n",
    "\n",
    "• Learning Rate ($\\alpha$): This is a hyperparameter that controls how large of a step you take in the direction of the negative gradient. A smaller learning rate means more precise but slower convergence, while a larger learning rate might speed up convergence but risks overshooting the minimum.\n",
    "\n",
    "• Update Rule: The parameters are updated iteratively using the rule:\n",
    "   $$\n",
    "   \\theta := \\theta - \\alpha \\nabla J(\\theta)\n",
    "   $$\n",
    "   This means you adjust the parameters by subtracting the product of the learning rate and the gradient.\n",
    "\n",
    "• Convergence: The process continues until the changes in the objective function are sufficiently small, indicating that you have reached (or are very close to) a local minimum.\n",
    "\n",
    "Gradient descent can be further refined into variations like stochastic gradient descent (SGD) and mini-batch gradient descent, which can help with convergence speed and handling large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation Algorithm\n",
    "\n",
    "1. Initialize Parameters:\n",
    "   - Set the learning rate $\\eta$.\n",
    "   - Initialize weights $W$ and biases $b$ for the network.\n",
    "\n",
    "2. Set Hyperparameters:\n",
    "   - Define the number of epochs (e.g., 5).\n",
    "\n",
    "3. Training Loop:\n",
    "   - For each epoch:\n",
    "     1. Initialize epoch loss: Set $epoch\\_loss = 0$ for tracking the average loss.\n",
    "     \n",
    "     2. Loop through each training example:\n",
    "        - Randomly select one input example $x$ from the dataset $X$.\n",
    "        - Get the corresponding true label $y_{true}$.\n",
    "\n",
    "        3. Forward Propagation:\n",
    "           - Compute the predicted output $y_{pred}$ by passing $x$ through the network.\n",
    "\n",
    "        4. Calculate Loss:\n",
    "           - Use the Mean Squared Error (MSE) to calculate the loss:\n",
    "             $loss = \\frac{1}{n} \\sum (y_{true} - y_{pred})^2$\n",
    "\n",
    "        5. Backward Propagation:\n",
    "           - Calculate gradients of the loss with respect to weights and biases.\n",
    "           - Update weights using the formula:\n",
    "             $W = W - \\eta \\frac{\\partial L}{\\partial W}$\n",
    "           - Repeat for all weights and biases.\n",
    "\n",
    "        6. Accumulate Loss:\n",
    "           - Add the current loss to $epoch\\_loss$.\n",
    "\n",
    "     3. Calculate Average Loss:\n",
    "        - Compute the average loss for the epoch:\n",
    "          $avg\\_epoch\\_loss = \\frac{epoch\\_loss}{n}$\n",
    "\n",
    "     4. Print Progress:\n",
    "        - Output the current epoch number and average loss.\n",
    "\n",
    "Summary of Algorithm\n",
    "\n",
    "1. Initialize parameters (weights $W$, biases $b$, learning rate $\\eta$).\n",
    "2. For each epoch:\n",
    "   - Initialize $epoch\\_loss$.\n",
    "   - For each training example:\n",
    "     - Select a random input $x$ and its label $y_{true}$.\n",
    "     - Predict output $y_{pred}$ using forward propagation.\n",
    "     - Calculate loss using MSE.\n",
    "     - Compute gradients and update weights.\n",
    "     - Accumulate the loss.\n",
    "   - Calculate and print average loss for the epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "\n",
    "1. Three Variants Overview:\n",
    "- The image shows there are three variants of gradient descent\n",
    "- They differ in how much data is used to compute the gradient\n",
    "- These variants involve a fundamental trade-off between accuracy, time, and efficiency\n",
    "\n",
    "1. Batch Gradient Descent:\n",
    "- Uses the entire dataset to compute gradient in each iteration\n",
    "- Code shows straightforward implementation with `evaluate_gradient` on full data\n",
    "- Advantages:\n",
    "  - Most accurate gradient computation\n",
    "  - Stable convergence\n",
    "- Disadvantages:\n",
    "  - Slowest per iteration\n",
    "  - Memory intensive for large datasets\n",
    "\n",
    "1. Stochastic Gradient Descent (SGD):\n",
    "- Uses single random example per iteration\n",
    "- Code shows:\n",
    "  - Random shuffling of data\n",
    "  - Iterates through individual examples\n",
    "  - Updates parameters for each example\n",
    "- Advantages:\n",
    "  - Fastest per iteration\n",
    "  - Low memory requirements\n",
    "  - Can escape local minima better\n",
    "- Disadvantages:\n",
    "  - Noisier updates\n",
    "  - Less stable convergence\n",
    "\n",
    "1. Mini-batch Gradient Descent (not shown in code but implied):\n",
    "- Uses a small batch of examples\n",
    "- Compromise between batch and stochastic\n",
    "- Advantages:\n",
    "  - Better than batch GD for large datasets\n",
    "  - More stable than SGD\n",
    "  - Can leverage vectorization\n",
    "\n",
    "1. Trade-off Summary (as noted in image):\n",
    "- Accuracy ↔ Time ↔ Scale of data\n",
    "- More data for gradient computation = higher accuracy but slower updates\n",
    "- Less data = faster updates but potentially less accurate gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation Algorithm\n",
    "\n",
    "$epochs = 5$\n",
    "\n",
    "1. For $i$ in range($epochs$):\n",
    "   \n",
    "2. For $j$ in range($X.shape[0]$):\n",
    "    \n",
    "    → Select 1 row (random)\n",
    "    \n",
    "    → Predict (using forward prop)\n",
    "    \n",
    "    → Calculate loss (using loss function → $MSE$)\n",
    "    \n",
    "    → Update weights and bias using GD:\n",
    "       $W_{new} = W_{old} - \\eta(\\frac{\\partial L}{\\partial W})$\n",
    "\n",
    "3. Calculate avg loss for the epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
