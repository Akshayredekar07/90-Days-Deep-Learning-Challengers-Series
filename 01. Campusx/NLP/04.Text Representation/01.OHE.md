# **One-Hot Encoding**

## 1. Introduction to One-Hot Encoding
One-hot encoding is a method of representing words as binary vectors. Each word in the vocabulary is assigned a unique vector where only one element is 1 (indicating the presence of that word), and all other elements are 0. This technique allows us to convert categorical text data into a numerical format that machine learning algorithms can process.

## 2. Corpus and Documents
**Given Documents:**
* **D1:** people watch campusx
* **D2:** campusx watch campusx
* **D3:** people write comment
* **D4:** campusx write comment

**Forming the Corpus:**
To create our one-hot encoding, we first combine all documents into a single corpus:
```
Corpus = people watch campusx campusx watch campusx people write comment campusx write comment
```
This corpus contains all words from all documents, maintaining their original sequence, which gives us the complete set of words used across our dataset.

## 3. Vocabulary Creation
From the corpus, we extract the unique words to form our vocabulary:
```
Vocabulary = [people, watch, campusx, write, comment]
```

Total vocabulary size:
```
V = 5
```
The vocabulary size determines the dimensionality of our one-hot vectors. Each unique word in our corpus will be represented by a vector of length V.

## 4. One-Hot Encoding Vectors
Each word is represented as a one-hot vector of size **V = 5**, where exactly one position contains a 1 (corresponding to the word's position in the vocabulary) and all other positions contain 0s.

| Word | Vector | Explanation |
|------|--------|-------------|
| people | [1, 0, 0, 0, 0] | 1 in first position, 0s elsewhere |
| watch | [0, 1, 0, 0, 0] | 1 in second position, 0s elsewhere |
| campusx | [0, 0, 1, 0, 0] | 1 in third position, 0s elsewhere |
| write | [0, 0, 0, 1, 0] | 1 in fourth position, 0s elsewhere |
| comment | [0, 0, 0, 0, 1] | 1 in fifth position, 0s elsewhere |

## 5. Document Vector Representation
Each document is represented as a sequence of one-hot vectors, forming a matrix where each row corresponds to a word in the document.

**Document D1:** "people watch campusx"
```
D1 = [
   [1, 0, 0, 0, 0],  # people
   [0, 1, 0, 0, 0],  # watch
   [0, 0, 1, 0, 0]   # campusx
]
Shape = (3, 5)
```

**Document D2:** "campusx watch campusx"
```
D2 = [
   [0, 0, 1, 0, 0],  # campusx
   [0, 1, 0, 0, 0],  # watch
   [0, 0, 1, 0, 0]   # campusx
]
Shape = (3, 5)
```

**Document D3:** "people write comment"
```
D3 = [
   [1, 0, 0, 0, 0],  # people
   [0, 0, 0, 1, 0],  # write
   [0, 0, 0, 0, 1]   # comment
]
Shape = (3, 5)
```

**Document D4:** "campusx write comment"
```
D4 = [
   [0, 0, 1, 0, 0],  # campusx
   [0, 0, 0, 1, 0],  # write
   [0, 0, 0, 0, 1]   # comment
]
Shape = (3, 5)
```

## 6. Matrix Shape
Each document is represented as a matrix with the shape:
```
(number of words in document, vocabulary size)
```

Examples:
* D1 has 3 words → shape = (3, 5)
* D2 has 3 words → shape = (3, 5)
* D3 has 3 words → shape = (3, 5)
* D4 has 3 words → shape = (3, 5)

This matrix representation allows us to work with text data in a numerical format, enabling the application of various machine learning algorithms.

## 7. Pros & Flaws of One-Hot Encoding

### Pros of One-Hot Encoding
1. **Intuitive**
   * Very simple to understand.
   * Each word is uniquely identified by a binary vector.
   * No assumptions about word relationships are imposed by the encoding.

2. **Easy to Implement**
   * Doesn't require complex preprocessing or training.
   * Can be quickly applied to any text corpus.
   * Works well for small vocabularies and initial NLP tasks.

### Flaws of One-Hot Encoding
1. **Sparsity**
   * Most of the elements in the vector are zeros.
   * This leads to inefficient memory usage (sparse array problem).
   * For large vocabularies, this becomes computationally expensive.

2. **No Fixed Size**
   * Vocabulary size (`V`) defines the vector dimension.
   * If vocabulary increases, the vector size also increases → scalability issue.
   * Makes it difficult to add new words without retraining the entire model.

3. **OOV (Out-of-Vocabulary)**
   * If an unknown word appears during prediction (not in training), we can't encode it.
   * Requires special handling of words not seen during training.

4. **No Semantic Meaning Captured**
   * Similar words like "walk" and "run" are orthogonal in vector space → no relationship is encoded.
   * Fails to capture linguistic or semantic relationships between words.
   * Each word is equally distant from every other word regardless of meaning.

## 8. Visual Representation
To illustrate the limitation of no semantic meaning, consider a simplified example with only three words:

| Word | Vector |
|------|--------|
| walk | [1, 0, 0] |
| run | [0, 1, 0] |
| shoe | [0, 0, 1] |

Vector space characteristics:
* Vectors are at 90° angles (orthogonal) to each other.
* Distance between every pair is the same (root 2).
* Semantically similar words like **walk** and **run** are treated the same as **walk** and **shoe**.
* This fails to capture the intuitive similarity that "walk" and "run" are more related to each other than either is to "shoe".

## 9. Common Concerns & Questions

### Concern 1: "Sparse Array Overfitting"
* Since the one-hot vector is mostly 0s, the model may struggle to generalize well and may **overfit** on small training data.
* High-dimensional sparse data increases computation and makes learning harder for complex models.
* As vocabulary grows, the curse of dimensionality becomes more pronounced.

### Concern 2: What if D3 = `people write comment matchs`?
**Issue:**
* The word **"matchs"** is not in the vocabulary.
* This is an **Out-of-Vocabulary (OOV)** problem.
* One-hot encoding cannot handle this unless we retrain with the new vocabulary or use a placeholder like `<UNK>`.
* We would need to decide whether to ignore the word, create a new vector dimension, or map it to a special "unknown" token.

### Concern 3: What if at prediction time D = `hello campusx peoples`?
**Issues:**
* **"hello"** and **"peoples"** are new words not seen during training → again an **OOV** problem.
* One-hot encoding has **no mechanism to infer or approximate meaning** of unknown words.
* Even though "peoples" is a plural form of "people" which is in our vocabulary, one-hot encoding treats them as completely unrelated.
* This highlights the need for more sophisticated word representation techniques.

## Summary
* One-hot encoding is a fundamental technique for representing words numerically in NLP.
* Vocabulary size determines the vector size for all word representations.
* Each word vector has a single '1' and the rest are '0'.
* Each document becomes a matrix of these vectors, with shape (document length, vocabulary size).
* While straightforward to implement, one-hot encoding suffers from issues of sparsity, fixed vocabulary, out-of-vocabulary words, and inability to capture word relationships.
* More advanced techniques like word embeddings (Word2Vec, GloVe) and contextual embeddings (BERT, GPT) were developed to address these limitations.