# TF-IDF (Term Frequency–Inverse Document Frequency)

## Objective of TF-IDF

TF-IDF is a technique used in Natural Language Processing (NLP) to convert text documents into meaningful numerical vectors while highlighting important words and down-weighting common ones. This statistical measure helps in:
- Quantifying the importance of words in documents
- Facilitating document comparison and similarity analysis
- Serving as a core component in information retrieval systems
- Providing feature vectors for text classification tasks

## Dataset Overview

The example uses 4 documents:
```
D1: people watch campusx  
D2: campusx watch campusx  
D3: people write comment  
D4: campusx write comment  
```

The vocabulary extracted from all documents is:
```
["people", "watch", "campusx", "write", "comment"]
```

The final vector dimension is 5D (one for each unique word), creating a sparse representation where many values may be zero.

## Step 1: Term Frequency (TF)

The Term Frequency (TF) of a term `t` in a document `d` is defined as:

TF(t, d) = Number of times term t appears in d / Total number of terms in d

**Example for D1 = "people watch campusx"** (each word appears once, total terms = 3):
- TF(people, D1) = 1/3  
- TF(watch, D1) = 1/3  
- TF(campusx, D1) = 1/3

**Example for D2 = "campusx watch campusx"** (note "campusx" appears twice):
- TF(people, D2) = 0/3 = 0
- TF(watch, D2) = 1/3
- TF(campusx, D2) = 2/3
- TF(write, D2) = 0
- TF(comment, D2) = 0

**Important Notes:**
- TF values are normalized between 0 and 1
- TF alone does not distinguish between important vs common words
- TF captures the frequency but not the significance of terms
- Higher TF values indicate more frequent terms within a specific document

## Step 2: Inverse Document Frequency (IDF)

The IDF of a term `t` is calculated as:

IDF(t) = log_e(Total number of documents / Number of documents containing term t)

Or more compactly:

IDF(t) = log_e(N / df_t)

Where:
- N = total number of documents = 4
- df_t = number of documents in which term `t` appears

**Examples:**
- "people" → appears in D1 and D3 → df = 2 → IDF = log(4/2) = log(2) ≈ 0.693
- "campusx" → appears in D1, D2, D4 → df = 3 → IDF = log(4/3) ≈ 0.288
- "write" → appears in D3, D4 → df = 2 → IDF = log(4/2) = log(2) ≈ 0.693
- "watch" → appears in D1, D2 → df = 2 → IDF = log(4/2) = log(2) ≈ 0.693
- "comment" → appears in D3, D4 → df = 2 → IDF = log(4/2) = log(2) ≈ 0.693

**Why Use Logarithm in IDF?**
- Without logarithm, rare terms would get disproportionately large weights
- The logarithm function smooths the scale, making values more manageable
- It helps prevent rare terms from dominating completely
- Words appearing in all documents get IDF = log(1) = 0, effectively removing them

## Step 3: TF-IDF Score Calculation

TF-IDF(t, d) = TF(t, d) × IDF(t)

Now multiply the TF values with the IDF values for each term-document pair.

**Example for Document D1:**
- TF-IDF(people, D1) = (1/3) × log(4/2) ≈ 0.231
- TF-IDF(watch, D1) = (1/3) × log(4/2) ≈ 0.231
- TF-IDF(campusx, D1) = (1/3) × log(4/3) ≈ 0.096
- TF-IDF(write, D1) = 0 (word not present)
- TF-IDF(comment, D1) = 0 (word not present)

**Example for Document D2:**
- TF-IDF(people, D2) = 0 (word not present)
- TF-IDF(watch, D2) = (1/3) × log(4/2) ≈ 0.231
- TF-IDF(campusx, D2) = (2/3) × log(4/3) ≈ 0.192
- TF-IDF(write, D2) = 0 (word not present)
- TF-IDF(comment, D2) = 0 (word not present)

## Final TF-IDF Matrix

| Term      | D1                      | D2                      | D3                      | D4                      |
|-----------|-------------------------|-------------------------|-------------------------|-------------------------|
| people    | 1/3 × log(2) ≈ 0.231    | 0                       | 1/3 × log(2) ≈ 0.231    | 0                       |
| watch     | 1/3 × log(2) ≈ 0.231    | 1/3 × log(2) ≈ 0.231    | 0                       | 0                       |
| campusx   | 1/3 × log(4/3) ≈ 0.096  | 2/3 × log(4/3) ≈ 0.192  | 0                       | 1/3 × log(4/3) ≈ 0.096  |
| write     | 0                       | 0                       | 1/3 × log(2) ≈ 0.231    | 1/3 × log(2) ≈ 0.231    |
| comment   | 0                       | 0                       | 1/3 × log(2) ≈ 0.231    | 1/3 × log(2) ≈ 0.231    |

## Document Representation as TF-IDF Vectors

Each document is now represented as a 5-dimensional vector:
- D1: [0.231, 0.231, 0.096, 0, 0]
- D2: [0, 0.231, 0.192, 0, 0]
- D3: [0.231, 0, 0, 0.231, 0.231]
- D4: [0, 0, 0.096, 0.231, 0.231]

## Interpretation and Intuition

- **TF** tells how often a word appears in a document (but doesn't capture uniqueness)
- **IDF** down-weights common words across documents (if a word appears in every document, IDF ≈ 0)
- **TF-IDF** emphasizes important, rare words in a document and is widely used in text classification, search engines, recommendation systems, etc.

## Observations and Insights

- Words like "campusx" that appear in many documents have low IDF, resulting in low TF-IDF scores
- Rare words that appear in fewer documents have higher IDF, resulting in higher TF-IDF scores when present in a document
- Notice that "campusx" in D2 has a higher TF-IDF value (0.192) than in D1 and D4 (0.096) because it appears twice in D2
- The resulting vectors are sparse (contain many zeros), making them storage-efficient
- Document similarity can be calculated using cosine similarity between these TF-IDF vectors

## Limitations of TF-IDF

- **Semantic blindness**: TF-IDF treats words as independent tokens and doesn't capture semantic relationships (e.g., "car" and "automobile" are treated as completely different)
- **Out-of-vocabulary problem**: Cannot handle new words not seen in the training corpus
- **Context insensitivity**: Doesn't consider word order or contextual meaning
- **Dimensionality issues**: As vocabulary grows, the vector dimension increases, leading to the curse of dimensionality

## Applications of TF-IDF

- Document search and retrieval systems
- Document clustering and classification
- Keyword extraction and text summarization
- Content recommendation engines
- Spam filtering and sentiment analysis

----
Here is the combined explanation with all the formulas:

---

### **Term Frequency-Inverse Document Frequency (TF-IDF)**

The **TF-IDF** is a statistical method used to evaluate the importance of a word within a document relative to a collection of documents, also called a **corpus**. The method combines two key metrics:

---

### 1. **Term Frequency (TF)**

The **Term Frequency (TF)** measures how often a specific word (or term) appears in a document. It quantifies the frequency of the term within the document. To prevent longer documents from automatically having higher values, this can be normalized by dividing the term count by the total number of words in the document.

#### Formula:
$$
\text{TF}(t,d) = \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}}
$$

Where:
- $ t $ = the term (word) whose frequency is being calculated.
- $ d $ = the document in which the term appears.
- $ f_{t,d} $ = the frequency (count) of term $ t $ in document $ d $.
- $ \sum_{t' \in d} f_{t',d} $ = the sum of frequencies of all terms in document $ d $ (i.e., the total number of terms in the document).

The TF formula calculates the relative frequency of a term $ t $ in document $ d $, divided by the total number of words in that document.

---

### 2. **Inverse Document Frequency (IDF)**

The **Inverse Document Frequency (IDF)** measures how rare or common a term is across all documents in the corpus. If a word appears in many documents, it is considered less important, and its IDF will be lower. In contrast, a word that appears in fewer documents will have a higher IDF.

#### Formula:
$$
\text{IDF}(t,D) = \log \left( \frac{N}{|\{ d \in D : t \in d \}|} \right)
$$

Where:
- $ t $ = the term (word) whose rarity is being calculated.
- $ D $ = the entire collection (corpus) of documents.
- $ N $ = the total number of documents in the corpus $ D $.
- $ |\{ d \in D : t \in d \}| $ = the number of documents in the corpus that contain the term $ t $.

The formula calculates the logarithm of the ratio between the total number of documents $ N $ and the number of documents containing the term $ t $. A higher IDF score indicates that the term is rare across documents, making it more significant.

---

### 3. **TF-IDF Score**

The **TF-IDF score** is the product of the **Term Frequency (TF)** and **Inverse Document Frequency (IDF)**. This score provides a measure of the importance of a term within a document in relation to the entire corpus. A high TF-IDF score means that the term is frequent in a particular document but rare across the corpus, making it important for that document.

#### Formula:
$$
\text{TF-IDF}(t,d,D) = \text{TF}(t,d) \times \text{IDF}(t,D)
$$

Where:
- $ \text{TF-IDF}(t,d,D) $ = the TF-IDF score of term $ t $ in document $ d $ within the corpus $ D $.
- $ \text{TF}(t,d) $ = the term frequency of $ t $ in $ d $.
- $ \text{IDF}(t,D) $ = the inverse document frequency of $ t $ in $ D $.

---

### Use of TF-IDF:
- **High TF-IDF**: Indicates that the word is frequent in a specific document but rare in others, making it important for that document.
- **Low TF-IDF**: Indicates that the word is common across many documents, and thus less important for distinguishing documents.

TF-IDF is commonly used in **search engines**, **document classification**, and **text mining** to identify important terms and filter out common words (e.g., "the", "and") that are not helpful for distinguishing between documents.

--- 
