# N-grams for Text Vectorization

## Basics of N-gram

An **n-gram** is a contiguous sequence of *n items* (usually words) from a given text sample. N-grams allow us to capture local word order and context, which is a significant improvement over the Bag of Words model.

N-grams are classified based on their size:
* **Unigram (1-gram)**: Single words (e.g., "people", "watch", "campusx")
* **Bigram (2-gram)**: Two consecutive words (e.g., "people watch", "watch campusx")
* **Trigram (3-gram)**: Three consecutive words (e.g., "people watch campusx")
* **Quadgram (4-gram)**: Four consecutive words (e.g., "people watch campusx write")

## Document Examples

| Document | Sentence | Label |
|----------|----------|-------|
| D1 | people watch campusx | 1 |
| D2 | campusx watch campusx | 1 |
| D3 | people write comment | 0 |
| D4 | campusx write comment | 0 |

## Bigram Vectorization Process

### Step 1: Extract All Possible Bigrams
From our documents, we extract the following bigrams:
```
1. people watch  
2. watch campusx  
3. campusx watch  
4. people write  
5. write comment  
6. campusx write
```

This gives us a **vocabulary size (V) = 6**.

### Step 2: Vector Representation
Each document is represented as a vector indicating the presence (1) or absence (0) of each bigram:

| Document | people watch | watch campusx | campusx watch | people write | write comment | campusx write |
|----------|--------------|---------------|---------------|--------------|---------------|---------------|
| D1       | 1            | 1             | 0             | 0            | 0             | 0             |
| D2       | 0            | 0             | 1             | 0            | 0             | 0             |
| D3       | 0            | 0             | 0             | 1            | 1             | 0             |
| D4       | 0            | 0             | 0             | 0            | 1             | 1             |

This transformation is called **Bag of Bigrams**.

## Quadgram Limitations

When attempting to create quadgrams (4-grams) with our example documents, we encounter a problem:

Each document has **only 3 words**, so:
* We **cannot extract any 4-word sequence**
* Any attempt to create quadgrams would result in an error or empty set

This highlights an important limitation: higher n-grams require documents with sufficient length to be effective.

## Comparing BoW vs N-gram with Example

### Example Sentences:
* `S1 = This movie is very good`
* `S2 = This movie is not good`

### Bag of Words Representation:

| Word | This | movie | is | very | good | not |
|------|------|-------|----|----|------|------|
| **S1** | 1 | 1 | 1 | 1 | 1 | 0 |
| **S2** | 1 | 1 | 1 | 0 | 1 | 1 |

Notice how both sentences have similar representations despite opposite meanings.

### Bigram Representation:

| Bigram | this movie | movie is | is very | very good | is not | not good |
|--------|------------|----------|---------|-----------|--------|----------|
| **S1** | 1 | 1 | 1 | 1 | 0 | 0 |
| **S2** | 1 | 1 | 0 | 0 | 1 | 1 |

The bigram representation better captures the semantic difference between the two sentences by preserving context.

## Advantages of N-gram Vectorization

1. **Preserves Local Word Order**
   * N-grams maintain the sequence of words within their local context
   * Captures phrases and word combinations that have specific meanings

2. **Better Semantic Understanding**
   * Can distinguish between "very good" and "not good" (unlike BoW)
   * Preserves contextual meaning of phrases

3. **Handles Negations and Modifiers**
   * Properly represents the effect of negation words and modifiers
   * Critical for sentiment analysis and opinion mining

## Limitations of N-gram Vectorization

1. **Increased Dimensionality**
   * Vocabulary size increases exponentially with n
   * For a vocabulary of size V, potential n-grams = V^n
   * Results in sparse matrices and high computational requirements

2. **Data Sparsity**
   * Many potential n-grams never occur in the corpus
   * Leads to very sparse vectors (mostly zeros)

3. **Out-of-Vocabulary (OOV) Issues**
   * N-grams not seen during training cannot be represented
   * More severe for higher values of n

4. **Document Length Requirements**
   * Higher values of n require longer documents
   * As seen with quadgrams in our example, documents must have at least n words

## Practical Recommendations

| Dataset Size | Recommended N-gram | Reason |
|--------------|-------------------|--------|
| Small | Unigrams | Low data, avoid overfitting |
| Medium | Bigrams | Some local context preserved |
| Large | Trigrams or higher | More data → better generalization, richer semantics |

## Comprehensive Comparison

| Criteria | BoW (Unigram) | N-gram (Bi/Trigram) |
|----------|---------------|---------------------|
| Word Order | ❌ Ignored | ✅ Partially preserved |
| Semantic Meaning | ❌ Lost | ✅ Retained |
| Dimensionality | ✅ Smaller | ❌ Grows exponentially with n |
| Implementation | ✅ Easy | ✅ Easy |
| Performance | ✅ Fast | ❌ Slower with large vocab |
| OOV Sensitivity | ✅ Lower (single words) | ❌ Higher (phrase combinations) |

## Summary

N-gram vectorization provides a more context-aware representation of text compared to simple Bag of Words, at the cost of increased dimensionality. The choice of n should be based on document length, dataset size, and the specific requirements of the NLP task.