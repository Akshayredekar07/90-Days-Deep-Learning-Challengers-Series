
### **Implement CNN from Scratch**

1. **Implement CNN:**
   - **Forward Propagation (FP) / Backward Propagation (BP):**
     - **Conv Layer**
     - **MaxPooling Layer**

2. **Architecture:**
   - Input: 28x28 (MNIST dataset)
   - Conv → MaxPooling → Flatten → Fully Connected Layers (Numpy)

---

### **Implement MLP**

1. **Forward Propagation (FP) / Backward Propagation (BP):**
   - Fully connected layers using NumPy for calculations.

2. **Dataset:**
   - MNIST Dataset

3. **Architecture:**
   - Input: Flattened 28x28 → Hidden Layers → Output (for classification)
  
---

### **2010-2012: Key Advancements in Deep Learning**

1. **Large Dataset Availability:**
   - **ImageNet:** 
     - 1000 categories, ~1 million images.
     - Enabled large-scale visual recognition tasks.

2. **Development of Complex Models:**
   - Models with **10^8 - 10^9 parameters** became common.
   - Incorporation of **CNNs**, **ReLU activations**, **Batch Normalization (BN)**, and **Pooling**, building on concepts from the 1990s.

3. **Use of GPUs for Deep Learning:**
   - **GPUs** were increasingly used to accelerate deep learning training, making it feasible to handle large datasets and complex models.

---

https://drive.google.com/file/u/0/d/1hHiOEBD0MRNZX7DwwShhHUsa5gLvVrPx

https://drive.google.com/file/u/0/d/1hHiOEBD0MRN2X7DwwShhHUsaSgLvVrPx/edit#scrollTo=hxzbqSf_4Dqy


https://colab.research.google.com/drive/1hHiOEBD0MRN2X7DwwShhHUsaSgLvVrPx